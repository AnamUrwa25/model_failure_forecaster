ğŸ“‰ MODEL FAILURE FORECASTER

An enterprise-grade Machine Learning monitoring dashboard for detecting model degradation, data drift, and prediction uncertainty.

ğŸ“Œ Project Overview

This project builds a universal monitoring system that works across domains such as:

âš¡ Electricity load forecasting

ğŸ¦ Banking transactions

ğŸ›’ E-commerce systems

ğŸ¥ Healthcare monitoring

ğŸ” Cybersecurity systems

The system detects when an ML model starts failing due to:

Prediction errors increasing

Input data distribution shifting

Model uncertainty rising

âš™ï¸ Features

âœ… Train ML model (Random Forest)

ğŸ“‰ Measure Prediction Error (MAE)

ğŸ”€ Detect Data Drift

â“ Estimate Model Uncertainty

ğŸ§  Compute Unified Health Score

ğŸ“Š Interactive Streamlit Dashboard

ğŸ“ Downloadable Health Reports

ğŸ“ˆ Cross-domain evaluation (Power & Bank datasets)

ğŸ“Š Experiments Conducted

Power Load Forecasting Dataset

Bank Marketing Dataset

Both datasets were used to evaluate:

Model Stability

Drift Behavior

Uncertainty Behavior

Health Score consistency

ğŸš€ How to Run
Step 1: Install Dependencies
pip install -r requirements.txt

Step 2: Run Streamlit Dashboard
streamlit run model_failure_forecaster.py

ğŸ“ Project Structure
model_failure_forecaster/
â”‚
â”œâ”€â”€ ABSTRACT.pdf
â”œâ”€â”€ model_failure_forecaster.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ notebooks/
â”‚     â””â”€â”€ model_failure_forecaster.ipynb
â”‚
â”œâ”€â”€ datasets/
â”‚     â”œâ”€â”€ power_load_data.csv
â”‚     â””â”€â”€ bank.csv
â”‚
â”œâ”€â”€ reports/
â”‚     â””â”€â”€ model_health_report.csv

